{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nThere are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.',\n",
       " AIMessage(content='There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 16, 'total_tokens': 43}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2e9ccd02-200b-49b5-9220-6469d39b5e18-0', usage_metadata={'input_tokens': 16, 'output_tokens': 27, 'total_tokens': 43}))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-1 LLM and Chat Models\n",
    "from langchain_openai import OpenAI, ChatOpenAI # LLM, Chat model\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "a = llm.invoke(\"How many planets are in the solar system?\")\n",
    "b = chat.invoke(\"How many planets are in the solar system?\")\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri. Come ti chiami?', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 53, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d65532a0-253a-4c36-8e87-308c9259c095-0', usage_metadata={'input_tokens': 53, 'output_tokens': 31, 'total_tokens': 84})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2-2. Predict Messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "# HumanMessage - 인간이 작성하는 Message\n",
    "# AIMessage - AI에 의해서 보내지는 Message\n",
    "# SystemMessage - LLM에 설정들을 제공하기 위한 Message\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italian.\"),\n",
    "    AIMessage(content=\"Ciao, mi chiamo Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between the Mexico and Thailand. Also, what is your name?\"),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri. Come ti chiami?', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 53, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d65532a0-253a-4c36-8e87-308c9259c095-0', usage_metadata={'input_tokens': 53, 'output_tokens': 31, 'total_tokens': 84})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2-3. Prompt Templates\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# 일반 문자열 출력 예제\n",
    "# template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}\")\n",
    "# prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "# chat.predict(prompt)\n",
    "\n",
    "# 메시지 출력 예제\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a geography expert. And you only reply in {language}.\"),\n",
    "    AIMessagePromptTemplate.from_template(\"Ciao, mi chiamo {name}!\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"What is the distance between the {country_a} and {country_b}. Also, what is your name?\")\n",
    "]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "prompt = template.format_messages(language=\"Italian\", name=\"Paolo\", country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-4. OutputParser and LCEL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# 문자열 출력을 파싱하는 BaseOutputParser 확장하는 커스텀 OutputParser\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "\n",
    "# OutputParser 예제 (LCEL 적용 전)\n",
    "# p = CommaOutputParser()\n",
    "# messages = [\n",
    "#     SystemMessagePromptTemplate.from_template(\"You are a list gernerating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do Not reply with else.\"),\n",
    "#     HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "# ]\n",
    "# template = ChatPromptTemplate.from_messages(messages)\n",
    "# prompt = template.format_messages(max_items=10, question=\"What are the colors?\")\n",
    "# res = chat.invoke(prompt)\n",
    "# p.parse(res.content)\n",
    "\n",
    "# OutputParser 예제 (LCEL 적용 후)\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a list gernerating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do Not reply with else.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "template = ChatPromptTemplate.from_messages(messages)\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\":10, \n",
    "    \"question\":\"What are the colors?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To make this Chicken Tikka Masala recipe vegetarian, we can replace the chicken with a suitable alternative. Here's how you can modify the recipe:\\n\\n**Vegetarian Tikka Masala**\\n\\n**Ingredients:**\\n- 1 lb firm tofu, pressed and cut into bite-sized cubes\\n- 1 cup plain yogurt (you can use plant-based yogurt for a vegan version)\\n- 2 tablespoons lemon juice\\n- 2 teaspoons ground cumin\\n- 2 teaspoons paprika\\n- 1 teaspoon ground turmeric\\n- 1 teaspoon garam masala\\n- 1 teaspoon ground coriander\\n- 1 teaspoon chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 2 tablespoons vegetable oil\\n- 1 onion, finely chopped\\n- 3 cloves garlic, minced\\n- 1-inch piece of ginger, grated\\n- 1 can (14 oz) crushed tomatoes\\n- 1 cup coconut cream (or heavy cream alternative for a vegan version)\\n- Fresh cilantro, chopped (for garnish)\\n\\n**Instructions:**\\n1. In a bowl, combine yogurt, lemon juice, cumin, paprika, turmeric, garam masala, coriander, chili powder, salt, and pepper. Add the tofu cubes and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\\n\\n2. Preheat the oven to 400°F (200°C). Thread the marinated tofu onto skewers and place them on a baking sheet lined with foil. Bake for 20-25 minutes or until the tofu is lightly browned.\\n\\n3. In a large skillet, heat vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes. Add the garlic and ginger, and cook for another minute until fragrant.\\n\\n4. Stir in the crushed tomatoes and simmer for 10 minutes. Add the coconut cream and cooked tofu tikka pieces. Simmer for an additional 10 minutes, stirring occasionally.\\n\\n5. Taste and adjust seasoning if needed. Garnish with chopped cilantro before serving.\\n\\n6. Serve the Vegetarian Tikka Masala hot with steamed rice or naan bread.\\n\\nEnjoy your flavorful and aromatic Vegetarian Tikka Masala!\", response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-93f9ef73-d490-4866-878e-4be3b72f0756-0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-5. Chaining Chains\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "chef_message =  [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a world-class international chef. You create easy to fllow recipies for any type of cuisine with easy to find ingredients.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"I want to cook {cuisine} food.\")\n",
    "]\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages(chef_message)\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "veg_chef_message =  [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a vegetarian chef specialized on marking tranditional recipies vegetarian. You find alternatibe ingredients and explain their preparation. You don't redically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{recipe}\")\n",
    "]\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages(veg_chef_message)\n",
    "\n",
    "veg_chef_chain = veg_chef_prompt | chat\n",
    "\n",
    "# RunnableMap 사용\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chef_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\":\"indian\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: \\n        I\\'m sorry, I\\'m not sure what you mean by \"Germ.\" Did you mean Germany?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-a639704b-2eb3-4730-8a41-3c3a437becde-0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1. FewShotPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# 모델에게 전달하는 답변 예제\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    }]\n",
    "\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt, # Prompt 방식\n",
    "    examples=examples, # 답변 예제\n",
    "    suffix=\"Human: Wat do you know about {country}?\", # 모든 형식화된 예제 마지막 내용\n",
    "    input_variables=[\"country\"] # suffix 입력 변수 (유효성 검사)\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Germ\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n        I know this:\\n        Capital: Berlin\\n        Language: German\\n        Food: Bratwurst and Sauerkraut\\n        Currency: Euro\\n        ', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-213f3cfb-0c41-4b51-aab7-5925964cf6d6-0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-2. FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# 모델에게 전달하는 답변 예제\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    }]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"What do you know about {country}?\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{answer}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt, # Prompt 방식\n",
    "    examples=examples, # 답변 예제\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a geography expert, you give short answers\"),\n",
    "    prompt,\n",
    "    HumanMessagePromptTemplate.from_template(\"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Germany\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuman: Wat do you know about Brazil?'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-3. LengthBasedExampleSelector\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# 모델에게 전달하는 답변 예제\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    }]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# 예제 선택\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples, # 답변 예제\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt, # Prompt 방식\n",
    "    example_selector=example_selector, # 답변 선택\n",
    "    suffix=\"Human: Wat do you know about {country}?\", # 모든 형식화된 예제 마지막 내용\n",
    "    input_variables=[\"country\"] # suffix 입력 변수 (유효성 검사)\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrrrg! Me favorite food be a hearty stew made with fresh seafood and plenty of spices! Aye, it be a meal fit for a pirate like meself! Arrrrg!', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-24515c0d-3d20-4b5c-8ffb-4b6a73447353-0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-4. Serialization and Composition\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# Prompt 파일 불러오기\n",
    "# from langchain.prompts import load_prompt\n",
    "# Prompt Pipeline 불러오기\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "# Prompt 파일 불러오기\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "# prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# Prompt 파일 불러오기\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "# full_prompt.format(character=\"Pirate\", example_question=\"What is your location?\", example_answer=\"Arrrrg! That is a secret!! Arg arg!!\", question=\"What is your fav food?\")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\":\"Pirate\",\n",
    "    \"example_question\":\"What is your location?\",\n",
    "    \"example_answer\":\"Arrrrg! That is a secret!! Arg arg\",\n",
    "    \"question\": \"What is your fav food?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss with your favorite sauce or toppings.\\n10. Serve hot and enjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-5. Caching\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# 메모리 캐시\n",
    "# set_llm_cache(InMemoryCache())\n",
    "# set_debug(False)\n",
    "\n",
    "# 데이터베이스 캐시\n",
    "set_llm_cache(SQLiteCache('cache.db'))\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    # streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 450}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seo/project/gpt/env/lib/python3.12/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/seo/project/gpt/env/lib/python3.12/site-packages/langchain_community/llms/openai.py:1072: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3-6. Serialization\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "# )\n",
    "\n",
    "# 지출 비용 확인\n",
    "# with get_openai_callback() as usage:\n",
    "#     a = chat.predict(\"What is the recipe for soju?\")\n",
    "#     b = chat.predict(\"What is the recipe for bread?\")\n",
    "#     print(a, b, \"\\n\")\n",
    "#     print(usage)\n",
    "\n",
    "# Serialization\n",
    "# 모델 저장\n",
    "# chat = OpenAI(\n",
    "#     temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "#     max_tokens=450,\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "# )\n",
    "\n",
    "# chat.save('model.json')\n",
    "\n",
    "# 모델 불러오기\n",
    "chat = load_llm('model.json')\n",
    "\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-1. ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 쳇 모델 용으로 사용할 경우 return_messages=True로 설정\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-2. ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 쳇 모델 용으로 사용할 경우 return_messages=True로 설정, K는 메모리의 크기\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "add_message(5, 5)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as living in South Korea, and the AI responds by mentioning it is a robot living in the cloud. The human comments on the beauty of South Korea, and the AI expresses a wish to visit there.'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-3. ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=chat)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"HI I am a human, I live in South Korea\", \"Wow that is cool! I am a robot living in the cloud.\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could visit there.\")\n",
    "\n",
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as living in South Korea. The AI responds by mentioning it is a robot living in the cloud and expresses a desire to visit South Korea because it is so pretty.')]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-4. ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"HI I am a human, I live in South Korea\", \"Wow that is cool! I am a robot living in the cloud.\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could visit there.\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Lee: Lee lives in South Korea. Lee likes kimchi.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-5. ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationKGMemory(\n",
    "    llm=chat,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"HI I am a Lee, I live in South Korea\", \"Wow that is cool! I am a robot living in the cloud.\")\n",
    "add_message(\"Lee likes kimchi\", \"wow that is cool!\")\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"who is Lee\"})\n",
    "memory.load_memory_variables({\"input\": \"what does Lee like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Seo\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Seo\n",
      "AI: Hello Seo! How can I assist you today?\n",
      "    Human:I live Seoul in South Korea\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Seo\n",
      "AI: Hello Seo! How can I assist you today?\n",
      "Human: I live Seoul in South Korea\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul or South Korea?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: My name is Seo\\nAI: Hello Seo! How can I assist you today?\\nHuman: I live Seoul in South Korea\\nAI: That's great to know! How can I assist you with information or tasks related to Seoul or South Korea?\\nHuman: What is my name?\\nAI: Your name is Seo.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-6. Memory on LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Seo\")\n",
    "chain.predict(question=\"I live Seoul in South Korea\") \n",
    "chain.predict(question=\"What is my name?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Seo\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Seo\n",
      "AI: Nice to meet you, Seo! How can I assist you today?\n",
      "Human: I live Seoul in South Korea\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Seo\n",
      "AI: Nice to meet you, Seo! How can I assist you today?\n",
      "Human: I live Seoul in South Korea\n",
      "AI: That's great to know, Seo! Is there anything specific you would like to know or talk about regarding Seoul or South Korea?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Seo.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-7. Chat Based Memory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # 많은 메시지들 사이를 구분하기 위한 placeholder\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Seo\")\n",
    "chain.predict(question=\"I live Seoul in South Korea\") \n",
    "chain.predict(question=\"What is my name?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Seo.', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 96, 'total_tokens': 102}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ed30c92a-055d-47bc-b4aa-e56cbd8d5508-0', usage_metadata={'input_tokens': 96, 'output_tokens': 6, 'total_tokens': 102})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-8. LCEL Based Memory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # 많은 메시지들 사이를 구분하기 위한 placeholder\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | chat\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "    return result\n",
    "\n",
    "invoke_chain(\"My name is Seo\")\n",
    "invoke_chain(\"I live Seoul in South Korea\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 699, which is longer than the specified 600\n",
      "Created a chunk of size 793, which is longer than the specified 600\n",
      "Created a chunk of size 845, which is longer than the specified 600\n",
      "Created a chunk of size 1236, which is longer than the specified 600\n",
      "Created a chunk of size 775, which is longer than the specified 600\n",
      "Created a chunk of size 2532, which is longer than the specified 600\n",
      "Created a chunk of size 671, which is longer than the specified 600\n",
      "Created a chunk of size 718, which is longer than the specified 600\n",
      "Created a chunk of size 1388, which is longer than the specified 600\n",
      "Created a chunk of size 892, which is longer than the specified 600\n",
      "Created a chunk of size 1861, which is longer than the specified 600\n",
      "Created a chunk of size 635, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 1701, which is longer than the specified 600\n",
      "Created a chunk of size 760, which is longer than the specified 600\n",
      "Created a chunk of size 982, which is longer than the specified 600\n",
      "Created a chunk of size 641, which is longer than the specified 600\n",
      "Created a chunk of size 1828, which is longer than the specified 600\n",
      "Created a chunk of size 822, which is longer than the specified 600\n",
      "Created a chunk of size 1086, which is longer than the specified 600\n",
      "Created a chunk of size 971, which is longer than the specified 600\n",
      "Created a chunk of size 665, which is longer than the specified 600\n",
      "Created a chunk of size 1563, which is longer than the specified 600\n",
      "Created a chunk of size 1055, which is longer than the specified 600\n",
      "Created a chunk of size 640, which is longer than the specified 600\n",
      "Created a chunk of size 1250, which is longer than the specified 600\n",
      "Created a chunk of size 1058, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-1. Data Loaders and Splitters\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# chunk_size - 텍스트를 분할하는 크기\n",
    "# chunk_overlap - 분할된 텍스트의 중복 크기\n",
    "# separator - 텍스트를 분할하는 구분자\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "len(loader.load_and_split(text_splitter=splitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1–1Introduction This two-year course in physics is presented from the point of view that you, the reader, are going to be a physicist. This is not necessarily the case of course, but that is what every professor in every subject assumes! If you are going to be a physicist, you will have a lot to study: two hundred years of the most rapidly developing field of knowledge that there is. So much knowledge, in fact, that you might think that you cannot learn all of it in four years, and truly you cannot; you will have to go to graduate school too!\\nSurprisingly enough, in spite of the tremendous amount of work that has been done for all this time it is possible to condense the enormous mass of results to a large extent—that is, to find laws which summarize all our knowledge. Even so, the laws are so hard to grasp that it is unfair to you to start exploring this tremendous subject without some kind of map or outline of the relationship of one part of the subject of science to another. Following these preliminary remarks, the first three chapters will therefore outline the relation of physics to the rest of the sciences, the relations of the sciences to each other, and the meaning of science, to help us develop a “feel” for the subject.\\nYou might ask why we cannot teach physics by just giving the basic laws on page one and then showing how they work in all possible circumstances, as we do in Euclidean geometry, where we state the axioms and then make all sorts of deductions. (So, not satisfied to learn physics in four years, you want to learn it in four minutes?) We cannot do it in this way for two reasons. First, we do not yet know all the basic laws: there is an expanding frontier of ignorance. Second, the correct statement of the laws of physics involves some very unfamiliar ideas which require advanced mathematics for their description. Therefore, one needs a considerable amount of preparatory training even to learn what the words mean. No, it is not possible to do it that way. We can only do it piece by piece.\\nEach piece, or part, of the whole of nature is always merely an approximation to the complete truth, or the complete truth so far as we know it. In fact, everything we know is only some kind of approximation, because we know that we do not know all the laws as yet. Therefore, things must be learned only to be unlearned again or, more likely, to be corrected.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Each piece, or part, of the whole of nature is always merely an approximation to the complete truth, or the complete truth so far as we know it. In fact, everything we know is only some kind of approximation, because we know that we do not know all the laws as yet. Therefore, things must be learned only to be unlearned again or, more likely, to be corrected.\\nThe principle of science, the definition, almost, is the following: The test of all knowledge is experiment. Experiment is the sole judge of scientific “truth.” But what is the source of knowledge? Where do the laws that are to be tested come from? Experiment, itself, helps to produce these laws, in the sense that it gives us hints. But also needed is imagination to create from these hints the great generalizations—to guess at the wonderful, simple, but very strange patterns beneath them all, and then to experiment to check again whether we have made the right guess. This imagining process is so difficult that there is a division of labor in physics: there are theoretical physicists who imagine, deduce, and guess at new laws, but do not experiment; and then there are experimental physicists who experiment, imagine, deduce, and guess.\\nWe said that the laws of nature are approximate: that we first find the “wrong” ones, and then we find the “right” ones. Now, how can an experiment be “wrong”? First, in a trivial way: if something is wrong with the apparatus that you did not notice. But these things are easily fixed, and checked back and forth. So without snatching at such minor things, how can the results of an experiment be wrong? Only by being inaccurate. For example, the mass of an object never seems to change: a spinning top has the same weight as a still one. So a “law” was invented: mass is constant, independent of speed. That “law” is now found to be incorrect. Mass is found to increase with velocity, but appreciable increases require velocities near that of light. A true law is: if an object moves with a speed of less than one hundred miles a second the mass is constant to within one part in a million. In some such approximate form this is a correct law. So in practice one might think that the new law makes no significant difference. Well, yes and no. For ordinary speeds we can certainly forget it and use the simple constant- mass law as a good approximation. But for high speeds we are wrong, and the higher the speed, the more wrong we are.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Finally, and most interesting, philosophically we are completely wrong with the approximate law. Our entire picture of the world has to be altered even though the mass changes only by a little bit. This is a very peculiar thing about the philosophy, or the ideas, behind the laws. Even a very small effect sometimes requires profound changes in our ideas.\\nNow, what should we teach first? Should we teach the correct but unfamiliar law with its strange and difficult conceptual ideas, for example the theory of relativity, four-dimensional space-time, and so on? Or should we first teach the simple “constant-mass” law, which is only approximate, but does not involve such difficult ideas? The first is more exciting, more wonderful, and more fun, but the second is easier to get at first, and is a first step to a real understanding of the first idea. This point arises again and again in teaching physics. At different times we shall have to resolve it in different ways, but at each stage it is worth learning what is now known, how accurate it is, how it fits into everything else, and how it may be changed when we learn more.\\nLet us now proceed with our outline, or general map, of our understanding of science today (in particular, physics, but also of other sciences on the periphery), so that when we later concentrate on some particular point we will have some idea of the background, why that particular point is interesting, and how it fits into the big structure. So, what is our overall picture of the world?\\n1–2Matter is made of atoms If, in some cataclysm, all of scientific knowledge were to be destroyed, and only one sentence passed on to the next generations of creatures, what statement would contain the most information in the fewest words? I believe it is the atomic hypothesis (or the atomic fact, or whatever you wish to call it) that all things are made of atoms—little particles that move around in perpetual motion, attracting each other when they are a little distance apart, but repelling upon being squeezed into one another. In that one sentence, you will see, there is an enormous amount\\nof information about the world, if just a little imagination and thinking are applied.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='of information about the world, if just a little imagination and thinking are applied.\\nFigure 1–1 To illustrate the power of the atomic idea, suppose that we have a drop of water a quarter of an inch on the side. If we look at it very closely we see nothing but water—smooth, continuous water. Even if we magnify it with the best optical microscope available—roughly two thousand times— then the water drop will be roughly forty feet across, about as big as a large room, and if we looked rather closely, we would still see relatively smooth water—but here and there small football-shaped things swimming back and forth. Very interesting. These are paramecia. You may stop at this point and get so curious about the paramecia with their wiggling cilia and twisting bodies that you go no further, except perhaps to magnify the paramecia still more and see inside. This, of course, is a subject for biology, but for the present we pass on and look still more closely at the water material itself, magnifying it two thousand times again. Now the drop of water extends about fifteen miles across, and if we look very closely at it we see a kind of teeming, something which no longer has a smooth appearance—it looks something like a crowd at a football game as seen from a very great distance. In order to see what this teeming is about, we will magnify it another two hundred and fifty times and we will see something similar to what is shown in Fig. 1–1. This is a picture of water magnified a billion times, but idealized in several ways. In the first place, the particles are drawn in a simple manner with sharp edges, which is inaccurate. Secondly, for simplicity, they are sketched almost schematically in a two-dimensional arrangement, but of course they are moving around in three dimensions. Notice that there are two kinds of “blobs” or circles to represent the atoms of oxygen (black) and hydrogen (white), and that each oxygen has two hydrogens tied to it. (Each little group of an oxygen with its two hydrogens is called a molecule.) The picture is idealized further in that the real particles in nature are continually jiggling and bouncing, turning and twisting around one another. You will have to imagine this as a dynamic rather than a static picture. Another thing that cannot be illustrated in a drawing is the fact that the particles are “stuck together”—that they attract each other, this one pulled by that one, etc. The whole group is “glued together,” so to speak. On the other hand, the particles do not squeeze through each other. If you try to squeeze two of them too close together, they repel.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='The atoms are 1 or 2×10−8 cm in radius. Now 10−8 cm is called an angstrom (just as another name), so we say they are 1 or 2 angstroms (Å) in radius. Another way to remember their size is this: if an apple is magnified to the size of the earth, then the atoms in the apple are approximately the size of the original apple.\\nNow imagine this great drop of water with all of these jiggling particles stuck together and tagging along with each other. The water keeps its volume; it does not fall apart, because of the attraction of the molecules for each other. If the drop is on a slope, where it can move\\nfrom one place to another, the water will flow, but it does not just disappear—things do not just fly apart—because of the molecular attraction. Now the jiggling motion is what we represent as heat: when we increase the temperature, we increase the motion. If we heat the water, the jiggling increases and the volume between the atoms increases, and if the heating continues there comes a time when the pull between the molecules is not enough to hold them together and they do fly apart and become separated from one another. Of course, this is how we manufacture steam out of water—by increasing the temperature; the particles fly apart because of the increased motion.\\nFigure 1–2 In Fig. 1–2 we have a picture of steam. This picture of steam fails in one respect: at ordinary atmospheric pressure there certainly would not be as many as three water molecules in this figure. Most squares this size would contain none—but we accidentally have two and a half or three in the picture (just so it would not be completely blank). Now in the case of steam we see the characteristic molecules more clearly than in the case of water. For simplicity, the molecules are drawn so that there is a 120∘ angle between the hydrogen atoms. In actual fact the angle is 105∘3′ , and the distance between the center of a hydrogen and the center of the oxygen is 0.957 Å, so we know this molecule very well.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Let us see what some of the properties of steam vapor or any other gas are. The molecules, being separated from one another, will bounce against the walls. Imagine a room with a number of tennis balls (a hundred or so) bouncing around in perpetual motion. When they bombard the wall, this pushes the wall away. (Of course we would have to push the wall back.) This means that the gas exerts a jittery force which our coarse senses (not being ourselves magnified a billion times) feel only as an average push. In order to confine a gas we must apply a pressure. Figure 1–3 shows a standard vessel for holding gases (used in all textbooks), a cylinder with a piston in it. Now, it makes no difference what the shapes of water molecules are, so for simplicity we shall draw them as tennis balls or little dots. These things are in perpetual motion in all directions. So many of them are hitting the top piston all the time that to keep it from being patiently knocked out of the tank by this continuous banging, we shall have to hold the piston down by a certain force, which we call the pressure (really, the pressure times the area is the force). Clearly, the force is proportional to the area, for if we increase the area but keep the number of molecules per cubic centimeter the same, we increase the number of collisions with the piston in the same proportion as the area was increased.\\nFigure 1–3 Now let us put twice as many molecules in this tank, so as to double the density, and let them have the same speed, i.e., the same temperature. Then, to a close approximation, the number of collisions will be doubled, and since each will be just as “energetic” as before, the pressure is proportional to the density. If we consider the true nature of the forces between the atoms, we would expect a slight decrease in pressure because\\nof the attraction between the atoms, and a slight increase because of the finite volume they occupy. Nevertheless, to an excellent approximation, if the density is low enough that there are not many atoms, the pressure is proportional to the density.\\nWe can also see something else: If we increase the temperature without changing the density of the gas, i.e., if we increase the speed of the atoms, what is going to happen to the pressure? Well, the atoms hit harder because they are moving faster, and in addition they hit more often, so the pressure increases. You see how simple the ideas of atomic theory are.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='We can also see something else: If we increase the temperature without changing the density of the gas, i.e., if we increase the speed of the atoms, what is going to happen to the pressure? Well, the atoms hit harder because they are moving faster, and in addition they hit more often, so the pressure increases. You see how simple the ideas of atomic theory are.\\nLet us consider another situation. Suppose that the piston moves inward, so that the atoms are slowly compressed into a smaller space. What happens when an atom hits the moving piston? Evidently it picks up speed from the collision. You can try it by bouncing a ping-pong ball from a forward-moving paddle, for example, and you will find that it comes off with more speed than that with which it struck. (Special example: if an atom happens to be standing still and the piston hits it, it will certainly move.) So the atoms are “hotter” when they come away from the piston than they were before they struck it. Therefore all the atoms which are in the vessel will have picked up speed. This means that when we compress a gas slowly, the temperature of the gas increases. So, under slow compression, a gas will increase in temperature, and under slow expansion it will decrease in temperature.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Figure 1–4 We now return to our drop of water and look in another direction. Suppose that we decrease the temperature of our drop of water. Suppose that the jiggling of the molecules of the atoms in the water is steadily decreasing. We know that there are forces of attraction between the atoms, so that after a while they will not be able to jiggle so well. What will happen at very low temperatures is indicated in Fig. 1–4: the molecules lock into a new pattern which is ice. This particular schematic diagram of ice is wrong because it is in two dimensions, but it is right qualitatively. The interesting point is that the material has a definite place for every atom, and you can easily appreciate that if somehow or other we were to hold all the atoms at one end of the drop in a certain arrangement, each atom in a certain place, then because of the structure of interconnections, which is rigid, the other end miles away (at our magnified scale) will have a definite location. So if we hold a needle of ice at one end, the other end resists our pushing it aside, unlike the case of water, in which the structure is broken down because of the increased jiggling so that the atoms all move around in different ways. The difference between solids and liquids is, then, that in a solid the atoms are arranged in some kind of an array, called a crystalline array, and they do not have a random position at long distances; the position of the atoms on one side of the crystal is determined by that of other atoms millions of atoms away on the other side of the crystal. Figure 1–4 is an invented arrangement for ice, and although it contains many of the correct features of ice, it is not the true arrangement. One of the correct features is that there is a part of the symmetry that is hexagonal. You can see that if we turn the picture around an axis by 60∘\\n, the picture returns to itself. So there is a symmetry in the ice which accounts for the six-sided appearance of snowflakes. Another thing we can see from Fig. 1–4 is why ice shrinks when it melts. The particular crystal pattern of ice shown here has many “holes” in it, as does the true ice structure. When the organization breaks down, these holes can be occupied by molecules. Most simple substances, with the exception of water and type metal, expand upon melting, because the atoms are closely packed in the solid crystal and upon melting need more room to jiggle around, but an open structure collapses, as in the case of water.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Now although ice has a “rigid” crystalline form, its temperature can change—ice has heat. If we wish, we can change the amount of heat. What is the heat in the case of ice? The atoms are not standing still. They are jiggling and vibrating. So even though there is a definite order to the crystal—a definite structure—all of the atoms are vibrating “in place.” As we increase the temperature, they vibrate with greater and greater amplitude, until they shake themselves out of place. We call this melting. As we decrease the temperature, the vibration decreases and decreases until, at absolute zero, there is a minimum amount of vibration that the atoms can have, but not zero. This minimum amount of motion that atoms can have is not enough to melt a substance, with one exception: helium. Helium merely decreases the atomic motions as much as it can, but even at absolute zero there is still enough motion to keep it from freezing. Helium, even at absolute zero, does not freeze, unless the pressure is made so great as to make the atoms squash together. If we increase the pressure, we can make it solidify.\\n1–3Atomic processes', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='1–3Atomic processes\\nFigure 1–5 So much for the description of solids, liquids, and gases from the atomic point of view. However, the atomic hypothesis also describes processes, and so we shall now look at a number of processes from an atomic standpoint. The first process that we shall look at is associated with the surface of the water. What happens at the surface of the water? We shall now make the picture more complicated—and more realistic—by imagining that the surface is in air. Figure 1–5 shows the surface of water in air. We see the water molecules as before, forming a body of liquid water, but now we also see the surface of the water. Above the surface we find a number of things: First of all there are water molecules, as in steam. This is water vapor, which is always found above liquid water. (There is an equilibrium between the steam vapor and the water which will be described later.) In addition we find some other molecules—here two oxygen atoms stuck together by themselves, forming an oxygen molecule, there two nitrogen atoms also stuck together to make a nitrogen molecule. Air consists almost entirely of nitrogen, oxygen, some water vapor, and lesser amounts of carbon dioxide, argon, and other things. So above the water surface is the air, a gas, containing some water vapor. Now what is happening in this picture? The molecules in the water are always jiggling around. From time to time, one on the surface happens to be hit a little harder than usual, and gets knocked away. It is hard to see that happening in the picture because it is a still picture. But we can imagine that one molecule near the surface has just been hit and is flying out, or perhaps another one has been hit and is\\nflying out. Thus, molecule by molecule, the water disappears—it evaporates. But if we close the vessel above, after a while we shall find a large number of molecules of water amongst the air molecules. From time to time, one of these vapor molecules comes flying down to the water and gets stuck again. So we see that what looks like a dead, uninteresting thing—a glass of water with a cover, that has been sitting there for perhaps twenty years—really contains a dynamic and interesting phenomenon which is going on all the time. To our eyes, our crude eyes, nothing is changing, but if we could see it a billion times magnified, we would see that from its own point of view it is always changing: molecules are leaving the surface, molecules are coming back.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Why do we see no change? Because just as many molecules are leaving as are coming back! In the long run “nothing happens.” If we then take the top of the vessel off and blow the moist air away, replacing it with dry air, then the number of molecules leaving is just the same as it was before, because this depends on the jiggling of the water, but the number coming back is greatly reduced because there are so many fewer water molecules above the water. Therefore there are more going out than coming in, and the water evaporates. Hence, if you wish to evaporate water turn on the fan!\\nHere is something else: Which molecules leave? When a molecule leaves it is due to an accidental, extra accumulation of a little bit more than ordinary energy, which it needs if it is to break away from the attractions of its neighbors. Therefore, since those that leave have more energy than the average, the ones that are left have less average motion than they had before. So the liquid gradually cools if it evaporates. Of course, when a molecule of vapor comes from the air to the water below there is a sudden great attraction as the molecule approaches the surface. This speeds up the incoming molecule and results in generation of heat. So when they leave they take away heat; when they come back they generate heat. Of course when there is no net evaporation the result is nothing—the water is not changing temperature. If we blow on the water so as to maintain a continuous preponderance in the number evaporating, then the water is cooled. Hence, blow on soup to cool it!\\nOf course you should realize that the processes just described are more complicated than we have indicated. Not only does the water go into the air, but also, from time to time, one of the oxygen or nitrogen molecules will come in and “get lost” in the mass of water molecules, and work its way into the water. Thus the air dissolves in the water; oxygen and nitrogen molecules will work their way into the water and the water will contain air. If we suddenly take the air away from the vessel, then the air molecules will leave more rapidly than they come in, and in doing so will make bubbles. This is very bad for divers, as you may know.\\nFigure 1–6\\nFigure 1–7 Now we go on to another process. In Fig. 1–6 we see, from an atomic point of view, a solid dissolving in water. If we put a crystal of salt in the water, what will happen? Salt is a solid, a crystal, an organized', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Figure 1–6\\nFigure 1–7 Now we go on to another process. In Fig. 1–6 we see, from an atomic point of view, a solid dissolving in water. If we put a crystal of salt in the water, what will happen? Salt is a solid, a crystal, an organized\\narrangement of “salt atoms.” Figure 1–7 is an illustration of the three- dimensional structure of common salt, sodium chloride. Strictly speaking, the crystal is not made of atoms, but of what we call ions. An ion is an atom which either has a few extra electrons or has lost a few electrons. In a salt crystal we find chlorine ions (chlorine atoms with an extra electron) and sodium ions (sodium atoms with one electron missing). The ions all stick together by electrical attraction in the solid salt, but when we put them in the water we find, because of the attractions of the negative oxygen and positive hydrogen for the ions, that some of the ions jiggle loose. In Fig. 1–6 we see a chlorine ion getting loose, and other atoms floating in the water in the form of ions. This picture was made with some care. Notice, for example, that the hydrogen ends of the water molecules are more likely to be near the chlorine ion, while near the sodium ion we are more likely to find the oxygen end, because the sodium is positive and the oxygen end of the water is negative, and they attract electrically. Can we tell from this picture whether the salt is dissolving in water or crystallizing out of water? Of course we cannot tell, because while some of the atoms are leaving the crystal other atoms are rejoining it. The process is a dynamic one, just as in the case of evaporation, and it depends on whether there is more or less salt in the water than the amount needed for equilibrium. By equilibrium we mean that situation in which the rate at which atoms are leaving just matches the rate at which they are coming back. If there is almost no salt in the water, more atoms leave than return, and the salt dissolves. If, on the other hand, there are too many “salt atoms,” more return than leave, and the salt is crystallizing.\\nIn passing, we mention that the concept of a molecule of a substance is only approximate and exists only for a certain class of substances. It is clear in the case of water that the three atoms are actually stuck together. It is not so clear in the case of sodium chloride in the solid. There is just an arrangement of sodium and chlorine ions in a cubic pattern. There is no natural way to group them as “molecules of salt.”', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='In passing, we mention that the concept of a molecule of a substance is only approximate and exists only for a certain class of substances. It is clear in the case of water that the three atoms are actually stuck together. It is not so clear in the case of sodium chloride in the solid. There is just an arrangement of sodium and chlorine ions in a cubic pattern. There is no natural way to group them as “molecules of salt.”\\nReturning to our discussion of solution and precipitation, if we increase the temperature of the salt solution, then the rate at which atoms are taken away is increased, and so is the rate at which atoms are brought back. It turns out to be very difficult, in general, to predict which way it is going to go, whether more or less of the solid will dissolve. Most substances dissolve more, but some substances dissolve less, as the temperature increases.\\n1–4Chemical reactions In all of the processes which have been described so far, the atoms and the ions have not changed partners, but of course there are circumstances in which the atoms do change combinations, forming new molecules. This is illustrated in Fig. 1–8. A process in which the rearrangement of the atomic partners occurs is what we call a chemical reaction. The other processes so far described are called physical processes, but there is no sharp distinction between the two. (Nature does not care what we call it, she just keeps on doing it.) This figure is supposed to represent carbon burning in oxygen. In the case of oxygen, two oxygen atoms stick together very strongly. (Why do not three or even four stick together? That is one of the very peculiar characteristics of such atomic processes. Atoms are\\nvery special: they like certain particular partners, certain particular directions, and so on. It is the job of physics to analyze why each one wants what it wants. At any rate, two oxygen atoms form, saturated and happy, a molecule.)', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='very special: they like certain particular partners, certain particular directions, and so on. It is the job of physics to analyze why each one wants what it wants. At any rate, two oxygen atoms form, saturated and happy, a molecule.)\\nFigure 1–8 The carbon atoms are supposed to be in a solid crystal (which could be graphite or diamond2). Now, for example, one of the oxygen molecules can come over to the carbon, and each atom can pick up a carbon atom and go flying off in a new combination—“carbon-oxygen”—which is a molecule of the gas called carbon monoxide. It is given the chemical name CO. It is very simple: the letters “CO” are practically a picture of that molecule. But carbon attracts oxygen much more than oxygen attracts oxygen or carbon attracts carbon. Therefore in this process the oxygen may arrive with only a little energy, but the oxygen and carbon will snap together with a tremendous vengeance and commotion, and everything near them will pick up the energy. A large amount of motion energy, kinetic energy, is thus generated. This of course is burning; we are getting heat from the combination of oxygen and carbon. The heat is ordinarily in the form of the molecular motion of the hot gas, but in certain circumstances it can be so enormous that it generates light. That is how one gets flames.\\nIn addition, the carbon monoxide is not quite satisfied. It is possible for it to attach another oxygen, so that we might have a much more complicated reaction in which the oxygen is combining with the carbon, while at the same time there happens to be a collision with a carbon monoxide molecule. One oxygen atom could attach itself to the CO and ultimately form a molecule, composed of one carbon and two oxygens, which is designated CO2 and called carbon dioxide. If we burn the carbon with very little oxygen in a very rapid reaction (for example, in an automobile engine, where the explosion is so fast that there is not time for it to make carbon dioxide) a considerable amount of carbon monoxide is formed. In many such rearrangements, a very large amount of energy is released, forming explosions, flames, etc., depending on the reactions. Chemists have studied these arrangements of the atoms, and found that every substance is some type of arrangement of atoms.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='To illustrate this idea, let us consider another example. If we go into a field of small violets, we know what “that smell” is. It is some kind of molecule, or arrangement of atoms, that has worked its way into our noses. First of all, how did it work its way in? That is rather easy. If the smell is some kind of molecule in the air, jiggling around and being knocked every which way, it might have accidentally worked its way into the nose. Certainly it has no particular desire to get into our nose. It is merely one helpless part of a jostling crowd of molecules, and in its aimless wanderings this particular chunk of matter happens to find itself in the nose.\\nFigure 1–9 Now chemists can take special molecules like the odor of violets, and analyze them and tell us the exact arrangement of the atoms in space. We\\nknow that the carbon dioxide molecule is straight and symmetrical: O—C—O. (That can be determined easily, too, by physical methods.) However, even for the vastly more complicated arrangements of atoms that there are in chemistry, one can, by a long, remarkable process of detective work, find the arrangements of the atoms. Figure 1–9 is a picture of the air in the neighborhood of a violet; again we find nitrogen and oxygen in the air, and water vapor. (Why is there water vapor? Because the violet is wet. All plants transpire.) However, we also see a “monster” composed of carbon atoms, hydrogen atoms, and oxygen atoms, which have picked a certain particular pattern in which to be arranged. It is a much more complicated arrangement than that of carbon dioxide; in fact, it is an enormously complicated arrangement. Unfortunately, we cannot picture all that is really known about it chemically, because the precise arrangement of all the atoms is actually known in three dimensions, while our picture is in only two dimensions. The six carbons which form a ring do not form a flat ring, but a kind of “puckered” ring. All of the angles and distances are known. So a chemical formula is merely a picture of such a molecule. When the chemist writes such a thing on the blackboard, he is trying to “draw,” roughly speaking, in two dimensions. For example, we see a “ring” of six carbons, and a “chain” of carbons hanging on the end, with an oxygen second from the end, three hydrogens tied to that carbon, two carbons and three hydrogens sticking up here, etc.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='Fig. 1–10.The substance pictured is α -irone. How does the chemist find what the arrangement is? He mixes bottles full of stuff together, and if it turns red, it tells him that it consists of one hydrogen and two carbons tied on here; if it turns blue, on the other hand, that is not the way it is at all. This is one of the most fantastic pieces of detective work that has ever been done—organic chemistry. To discover the arrangement of the atoms in these enormously complicated arrays the chemist looks at what happens when he mixes two different substances together. The physicist could never quite believe that the chemist knew what he was talking about when he described the arrangement of the atoms. For about twenty years it has been possible, in some cases, to look at such molecules (not quite as complicated as this one, but some which contain parts of it) by a physical method, and it has been possible to locate every atom, not by looking at colors, but by measuring where they are. And lo and behold!, the chemists are almost always correct.\\nIt turns out, in fact, that in the odor of violets there are three slightly different molecules, which differ only in the arrangement of the hydrogen atoms.\\nOne problem of chemistry is to name a substance, so that we will know what it is. Find a name for this shape! Not only must the name tell the shape, but it must also tell that here is an oxygen atom, there a hydrogen—exactly what and where each atom is. So we can appreciate that the chemical names must be complex in order to be complete. You see that the name of this thing in the more complete form that will tell you the structure of it is 4-(2, 2, 3, 6 tetramethyl-5-cyclohexenyl)-3-buten-2- one, and that tells you that this is the arrangement. We can appreciate the difficulties that the chemists have, and also appreciate the reason\\nfor such long names. It is not that they wish to be obscure, but they have an extremely difficult problem in trying to describe the molecules in words!', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='for such long names. It is not that they wish to be obscure, but they have an extremely difficult problem in trying to describe the molecules in words!\\nHow do we know that there are atoms? By one of the tricks mentioned earlier: we make the hypothesis that there are atoms, and one after the other results come out the way we predict, as they ought to if things are made of atoms. There is also somewhat more direct evidence, a good example of which is the following: The atoms are so small that you cannot see them with a light microscope—in fact, not even with an electron microscope. (With a light microscope you can only see things which are much bigger.) Now if the atoms are always in motion, say in water, and we put a big ball of something in the water, a ball much bigger than the atoms, the ball will jiggle around—much as in a push ball game, where a great big ball is pushed around by a lot of people. The people are pushing in various directions, and the ball moves around the field in an irregular fashion. So, in the same way, the “large ball” will move because of the inequalities of the collisions on one side to the other, from one moment to the next. Therefore, if we look at very tiny particles (colloids) in water through an excellent microscope, we see a perpetual jiggling of the particles, which is the result of the bombardment of the atoms. This is called the Brownian motion.\\nWe can see further evidence for atoms in the structure of crystals. In many cases the structures deduced by x-ray analysis agree in their spatial “shapes” with the forms actually exhibited by crystals as they occur in nature. The angles between the various “faces” of a crystal agree, within seconds of arc, with angles deduced on the assumption that a crystal is made of many “layers” of atoms.\\nEverything is made of atoms. That is the key hypothesis. The most important hypothesis in all of biology, for example, is that everything that animals do, atoms do. In other words, there is nothing that living things do that cannot be understood from the point of view that they are made of atoms acting according to the laws of physics. This was not known from the beginning: it took some experimenting and theorizing to suggest this hypothesis, but now it is accepted, and it is the most useful theory for producing new ideas in the field of biology.', metadata={'source': './files/chapter_one.pdf'}),\n",
       " Document(page_content='If a piece of steel or a piece of salt, consisting of atoms one next to the other, can have such interesting properties; if water—which is nothing but these little blobs, mile upon mile of the same thing over the earth—can form waves and foam, and make rushing noises and strange patterns as it runs over cement; if all of this, all the life of a stream of water, can be nothing but a pile of atoms, how much more is possible? If instead of arranging the atoms in some definite pattern, again and again repeated, on and on, or even forming little lumps of complexity like the odor of violets, we make an arrangement which is always different from place to place, with different kinds of atoms arranged in many ways, continually changing, not repeating, how much more marvelously is it possible that this thing might behave? Is it possible that that “thing” walking back and forth in front of you, talking to you, is a great glob of these atoms in a very complex arrangement, such that the sheer complexity of it staggers the imagination as to what it can do?\\nWhen we say we are a pile of atoms, we do not mean we are merely a pile of atoms, because a pile of atoms which is not repeated from one to the other might well have the possibilities which you see before you in the mirror.', metadata={'source': './files/chapter_one.pdf'})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-2. Tiktoken\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# chunk_size - 텍스트를 분할하는 크기\n",
    "# chunk_overlap - 분할된 텍스트의 중복 크기\n",
    "# separator - 텍스트를 분할하는 구분자\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-3. Vectors\n",
    "# Not Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-4. Vectors Store\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# chunk_size - 텍스트를 분할하는 크기\n",
    "# chunk_overlap - 분할된 텍스트의 중복 크기\n",
    "# separator - 텍스트를 분할하는 구분자\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cache_dir - 캐시 디렉토리\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 캐시된 임베딩을 사용하여 Vector Store 초기화\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# Vector Store 초기화\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# 유사도 검색\n",
    "result = vectorstore.similarity_search(\"what is introduction\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1–1Introduction This two-year course in physics is presented from the point of view that you, the reader, are going to be a physicist. This is not necessarily the case of course, but that is what every professor in every subject assumes! If you are going to be a physicist, you will have a lot to study: two hundred years of the most rapidly developing field of knowledge that there is. So much knowledge, in fact, that you might think that you cannot learn all of it in four years, and truly you cannot; you will have to go to graduate school too!\\nSurprisingly enough, in spite of the tremendous amount of work that has been done for all this time it is possible to condense the enormous mass of results to a large extent—that is, to find laws which summarize all our knowledge. Even so, the laws are so hard to grasp that it is unfair to you to start exploring this tremendous subject without some kind of map or outline of the relationship of one part of the subject of science to another. Following these preliminary remarks, the first three chapters will therefore outline the relation of physics to the rest of the sciences, the relations of the sciences to each other, and the meaning of science, to help us develop a “feel” for the subject.\\nYou might ask why we cannot teach physics by just giving the basic laws on page one and then showing how they work in all possible circumstances, as we do in Euclidean geometry, where we state the axioms and then make all sorts of deductions. (So, not satisfied to learn physics in four years, you want to learn it in four minutes?) We cannot do it in this way for two reasons. First, we do not yet know all the basic laws: there is an expanding frontier of ignorance. Second, the correct statement of the laws of physics involves some very unfamiliar ideas which require advanced mathematics for their description. Therefore, one needs a considerable amount of preparatory training even to learn what the words mean. No, it is not possible to do it that way. We can only do it piece by piece.\\nEach piece, or part, of the whole of nature is always merely an approximation to the complete truth, or the complete truth so far as we know it. In fact, everything we know is only some kind of approximation, because we know that we do not know all the laws as yet. Therefore, things must be learned only to be unlearned again or, more likely, to be corrected.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-5. Langsmith\n",
    "# Not Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Physics is the study of the fundamental principles that govern the natural world. It involves exploring the relationships between matter and energy, space and time, and the forces that act upon them. Physics is presented as a rapidly developing field of knowledge that has been studied for over two hundred years, condensing a large amount of results into laws that summarize our knowledge. The correct statement of the laws of physics involves unfamiliar ideas that require advanced mathematics for their description. Physics is considered to be an approximation to the complete truth, as our knowledge is always evolving and subject to correction.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-6. RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# chunk_size - 텍스트를 분할하는 크기\n",
    "# chunk_overlap - 분할된 텍스트의 중복 크기\n",
    "# separator - 텍스트를 분할하는 구분자\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cache_dir - 캐시 디렉토리\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 캐시된 임베딩을 사용하여 Vector Store 초기화\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# Vector Store 초기화\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\", # map_reduce, stuff\n",
    "    retriever=vectorstore.as_retriever(),  \n",
    ")\n",
    "\n",
    "chain.run(\"What is Physics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Physics is a rapidly developing field of knowledge that studies the fundamental principles of the universe, including matter, energy, motion, and force.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 2148, 'total_tokens': 2175}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5fc7e62-2709-4a2c-82ed-bec24e253b5a-0', usage_metadata={'input_tokens': 2148, 'output_tokens': 27, 'total_tokens': 2175})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-7. Stuff LCEL Chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# chunk_size - 텍스트를 분할하는 크기\n",
    "# chunk_overlap - 분할된 텍스트의 중복 크기\n",
    "# separator - 텍스트를 분할하는 구분자\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cache_dir - 캐시 디렉토리\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 캐시된 임베딩을 사용하여 Vector Store 초기화\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# Vector Store 초기화\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever();\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpfull assistant. Answer questions using only the following context. If you don't know the answer just say you don't knowm, don't make it up:\\n{context}\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = {\"context\": retriver, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "chain.invoke(\"What is Physics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-8. Map Reduce LCEL Chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# chunk_size - 텍스트를 분할하는 크기\n",
    "# chunk_overlap - 분할된 텍스트의 중복 크기\n",
    "# separator - 텍스트를 분할하는 구분자\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cache_dir - 캐시 디렉토리\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 캐시된 임베딩을 사용하여 Vector Store 초기화\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# Vector Store 초기화\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever();\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"\n",
    "    Use the following portion of a long document to see if any of the\n",
    "    text is relevant to answer the question. Return any relevant text\n",
    "    verbatim.\n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"question\": question, \"context\": doc.page_content}\n",
    "        ).content \n",
    "        for doc in documents\n",
    "    )\n",
    "    \n",
    "map_chain = {\"documents\": retriver, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs) \n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \n",
    "    \"\"\"\n",
    "    Given the following extracted parts of a long document and a \n",
    "    question, create a final answer.\n",
    "    If you don't know the answer, just say you don't know. Don't try\n",
    "    to make up an anwser.\n",
    "    {context}\n",
    "    \"\"\"\n",
    "     ),\n",
    "     (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"What is Physics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the meaning of potato?\\nThe word \"potato\" comes from the Spanish word \"patata,\" which was borrowed from the Taino language of the Caribbean. The Taino people called the potato \"batata,\" which referred to the sweet potato. The Spanish conquistador Hernán Cortés brought the potato back to Europe in the 16th century, where it became a staple crop.\\nThe meaning of potato is a type of root vegetable that is native to the Andean region of South America. It is a starchy vegetable that is high in carbohydrates and low in fat. Potatoes are a good source of fiber, potassium, and several important vitamins and minerals. They are also a versatile ingredient that can be prepared in a variety of ways, including baking, boiling, mashing, and frying.\\nIn addition to its culinary uses, the potato has also played an important role in human history and culture. It was a staple crop for many indigenous cultures in the Americas, and it played a key role in the development of European cuisine. The potato has also been a source of conflict and controversy, particularly in the 19th century when the Irish Potato Famine led to widespread poverty and emigration.\\nOverall, the meaning of potato is a complex and'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7-1. HuggingFaceHub\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is the meaning of {word}\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    # repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 250,\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A potato is a type of white potato with three-quarter legs.\\n\\nA yellowish, oval in shape, is actually a potato that comes from an animal.\\n\\nA potato has a large, yellowish outer shell and also has a thick outer shell.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7-2. HuggingFacePipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"openai-community/gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 50,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-3. GPT4ALL\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = GPT4All(model=\"./gpt4all-falcon-q4_0.gguf\", n_threads=1)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call an API...126.9779, 37.5665\n"
     ]
    }
   ],
   "source": [
    "# 8-8. Function Calling\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "# 날씨 함수 정의\n",
    "def get_weather(lon, lat):\n",
    "    print(f\"Call an API...{lon}, {lat}\")\n",
    "\n",
    "# 날씨 함수 스키마\n",
    "function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"function that takes longitude and latitude to find the weather of a place\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"lon\": {\"type\": \"string\", \"description\": \"The longitue coordinate\"},\n",
    "            \"lat\": {\"type\": \"string\", \"description\": \"The latitude coordinate\"},\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"lon\", \"lat\"]\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1  # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ").bind(\n",
    "    function_call=\"auto\",  # 자동으로 함수 호출\n",
    "    functions=[function],  # 함수 정의\n",
    ")\n",
    "\n",
    "# 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(\"Who is the weather in {city}\")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke({\"city\": \"Seoul\"})\n",
    "\n",
    "# 함수 호출\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "# JSON 파싱\n",
    "r = json.loads(response)\n",
    "\n",
    "# 함수 호출\n",
    "get_weather(**r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './files/podcast.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2023-05-05T21:58:50.000000Z\n",
      "  Duration: 01:13:42.43, start: 0.000000, bitrate: 267 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], 136 kb/s, 23.98 fps, 23.98 tbr, 24k tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2023-05-05T21:58:50.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/05/2023.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2023-05-05T21:58:50.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/05/2023.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> mp3 (libmp3lame))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp3, to './files/audio.mp3':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    TSSE            : Lavf61.1.100\n",
      "  Stream #0:0(eng): Audio: mp3, 44100 Hz, stereo, fltp (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2023-05-05T21:58:50.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/05/2023.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 libmp3lame\n",
      "[out#0/mp3 @ 0x142a049c0] video:0KiB audio:69101KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000488%\n",
      "size=   69101KiB time=01:13:42.42 bitrate= 128.0kbits/s speed= 160x    \n"
     ]
    }
   ],
   "source": [
    "# 10-1. Audio Extraction\n",
    "import subprocess\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    command = [\"ffmpeg\", \"-i\", video_path, \"-vn\", audio_path]\n",
    "    subprocess.run(command)\n",
    "\n",
    "extract_audio_from_video(\"./files/podcast.mp4\", \"./files/audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-2. Cutting The Audio\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# 오디오를 10분 단위로 자르기\n",
    "def cutting_audio_into_chunks(\n",
    "    audio_path,\n",
    "    chunks_path,\n",
    "    chunk_size=10,\n",
    "):\n",
    "    track = AudioSegment.from_file(audio_path)\n",
    "    ten_minutes = chunk_size * 60 * 1000\n",
    "    chunks = math.ceil(len(track) / ten_minutes)\n",
    "    for i in range(chunks):\n",
    "        start_time = i * ten_minutes\n",
    "        end_time = (i + 1) * ten_minutes\n",
    "        chunk = track[start_time:end_time]\n",
    "        chunk.export(f\"{chunks_path}/chunk_{i}.mp3\", format=\"mp3\")\n",
    "\n",
    "cutting_audio_into_chunks(\"./files/audio.mp3\", \"./files/chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-3. Whisper Transcript\n",
    "import openai\n",
    "import glob\n",
    "\n",
    "# Get Audio to Text\n",
    "def transcribe_chunks(chunk_path, destination):\n",
    "    files = glob.glob(f\"{chunk_path}/*.mp3\")\n",
    "    for file in files: \n",
    "        with open(file, \"rb\") as audio_file, open(destination, \"a\") as text_file:\n",
    "            # transcribe audio\n",
    "            transcript = openai.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", file=open(audio_file.name, \"rb\"), language=\"en\"\n",
    "            )\n",
    "            text_file.write(transcript.text)\n",
    "\n",
    "transcribe_chunks(\"./files/chunks\", \"./files/transcript.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use the Sum Calculator tool to find the total cost of all these amounts added together.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Sum Calculator\",\n",
      "  \"action_input\": {\"a\": 355.39, \"b\": 924.87}\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1280.26\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have calculated the sum of the first two amounts. Now, I will continue adding the remaining amounts to find the total cost.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Sum Calculator\",\n",
      "  \"action_input\": {\"a\": 1280.26, \"b\": 721.2}\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m2001.46\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have calculated the sum of the first three amounts. Now, I will continue adding the remaining amounts to find the total cost.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Sum Calculator\",\n",
      "  \"action_input\": {\"a\": 2001.46, \"b\": 1940.29}\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3941.75\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have calculated the sum of the first four amounts. Now, I will continue adding the remaining amounts to find the total cost.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12',\n",
       " 'output': 'I have calculated the sum of the first four amounts. Now, I will continue adding the remaining amounts to find the total cost.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11-1. Your First Agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1 # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "def plus(a, b):\n",
    "    return float(a) + float(b)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,  # LLM 모델\n",
    "    verbose=True,  # 상세 모드\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,  # 에이전트 타입\n",
    "    handle_parsing_errors=True,  # 파싱 에러 처리\n",
    "    tools=[\n",
    "        StructuredTool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. This tool take two arguments, both should be numbers.\",\n",
    "        )  # 함수 정의\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompot = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to add up all these numbers to find the total cost.\n",
      "Action: Sum Calculator\n",
      "Action Input: 355.39, 924.87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1280.26\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 1280.26, 721.2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m2001.46\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 2001.46, 1940.29\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3941.75\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 3941.75, 573.63\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4515.38\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 4515.38, 65.72\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4581.1\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 4581.1, 35.00\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4616.1\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 4616.1, 552.00\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5168.1\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum Calculator\n",
      "Action Input: 5168.1, 76.16\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5244.26\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to add the last number to the total to get the final answer.\n",
      "Action: Sum Calculator\n",
      "Action Input: 5244.26, 29.12\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5273.38\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: $5273.38\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12',\n",
       " 'output': '$5273.38'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11-3. Zero-shot ReAct Agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)  # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "\n",
    "\n",
    "def plus(input):\n",
    "    a, b = input.split(\",\")\n",
    "    return float(a) + float(b)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,  # LLM 모델\n",
    "    verbose=True,  # 상세 모드\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # 에이전트 타입\n",
    "    handle_parsing_errors=True,  # 파싱 에러 처리\n",
    "    tools=[\n",
    "        Tool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample: 1,2\",\n",
    "        )  # 함수 정의\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 355.39, 'b': 924.87}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m1280.26\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 1280.26, 'b': 721.2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2001.46\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 2001.46, 'b': 1940.29}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m3941.75\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 3941.75, 'b': 573.63}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m4515.38\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 4515.38, 'b': 65.72}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m4581.1\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 4581.1, 'b': 35.0}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m4616.1\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 4616.1, 'b': 552.0}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m5168.1\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 5168.1, 'b': 76.16}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m5244.26\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 5244.26, 'b': 29.12}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m5273.38\u001b[0m\u001b[32;1m\u001b[1;3mThe total cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12 is $5273.38.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12',\n",
       " 'output': 'The total cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12 is $5273.38.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11-3. OpenAI Functions Agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(temperature=0.1)  # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "\n",
    "# 계산기 툴 스키마\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "    a: float = Field(description=\"The first number\")\n",
    "    b: float = Field(description=\"The second number\")\n",
    "\n",
    "# 계산기 툴\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"CalculatorTool\"\n",
    "    description = \"\"\"\n",
    "    Use this to perform sums of two numbers.\n",
    "    The first and second arguments should be numbers.\n",
    "    Only receives two arguments.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CalculatorToolArgsSchema] = CalculatorToolArgsSchema\n",
    "\n",
    "    def _run(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "# 에이전트 초기화\n",
    "agent = initialize_agent(\n",
    "    llm=llm,  # LLM 모델\n",
    "    verbose=True,  # 상세 모드\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,  # 에이전트 타입\n",
    "    handle_parsing_errors=True,  # 파싱 에러 처리\n",
    "    tools=[CalculatorTool()],\n",
    ")\n",
    "\n",
    "# 프롬프트\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "# 에이전트 실행\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seo/project/fs_gpt/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `StockMarketSymbolSearchTool` with `{'query': 'Cloudflare'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRead the latest posts from Cloudflare experts and leaders on topics such as Internet traffic, cybersecurity, elections, AI, and more. Learn how Cloudflare products and features can help you protect, optimize, and innovate your online presence. Missed the live action at Cloudflare Connect? No worries! Dive into the exhilarating moments, groundbreaking insights, and industry revelations with our exclusive highlight reel. Immerse yourself in the energy of the event and stay tuned, stay connected, and stay inspired. Finally, because Cloudflare also operates a CDN, websites that are already on Cloudflare will be given a \"hot-path,\" and will load faster. We at Cloudflare are always striving to bring more privacy options to the open Internet, and we are excited to provide more private and secure browsing to Edge users. San Francisco, CA, May 2, 2024 — Cloudflare, Inc. (NYSE: NET), the leading connectivity cloud company, today announced financial results for its first quarter ended March 31, 2024. \"The first quarter marked a strong start to the year, as we grew revenue 30% year-over-year to $378.6 million—fueled by a record number of net-new customers ... Cloudflare Announces Fourth Quarter and Fiscal Year 2023 Financial Results. Feb 08, 2024. Cloudflare Announces Date of Fourth Quarter 2023 Financial Results. Jan 09, 2024. New Cloudflare Report Shows Organizations Struggle to Identify and Manage Cybersecurity Risks of APIs. Jan 09, 2024. We list global press coverage about Cloudflare on this page.\u001b[0m\u001b[32;1m\u001b[1;3mThe stock market symbol for Cloudflare is NET. Cloudflare, Inc. (NYSE: NET) is a leading connectivity cloud company that has shown strong growth in revenue. In the first quarter of 2024, Cloudflare grew revenue by 30% year-over-year to $378.6 million, fueled by a record number of net-new customers.\n",
      "\n",
      "To analyze if Cloudflare is a potential good investment, we can consider the following factors:\n",
      "1. **Financial Performance**: Cloudflare's strong revenue growth indicates a healthy financial performance.\n",
      "2. **Market Position**: Cloudflare is a leading connectivity cloud company, which positions it well in the market.\n",
      "3. **Industry Trends**: The increasing importance of cybersecurity and internet connectivity could benefit Cloudflare's business.\n",
      "4. **Competition**: It's important to assess how Cloudflare compares to its competitors in the market.\n",
      "5. **Future Growth Potential**: Analyzing Cloudflare's growth strategies and potential for future expansion can provide insights into its investment potential.\n",
      "\n",
      "It's recommended to conduct further research, consider your investment goals and risk tolerance before making any investment decisions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Give me information on Cloudflare stock and help me analyze if it's a potential good investment. Also tell me what symbol does the stock have.\",\n",
       " 'output': \"The stock market symbol for Cloudflare is NET. Cloudflare, Inc. (NYSE: NET) is a leading connectivity cloud company that has shown strong growth in revenue. In the first quarter of 2024, Cloudflare grew revenue by 30% year-over-year to $378.6 million, fueled by a record number of net-new customers.\\n\\nTo analyze if Cloudflare is a potential good investment, we can consider the following factors:\\n1. **Financial Performance**: Cloudflare's strong revenue growth indicates a healthy financial performance.\\n2. **Market Position**: Cloudflare is a leading connectivity cloud company, which positions it well in the market.\\n3. **Industry Trends**: The increasing importance of cybersecurity and internet connectivity could benefit Cloudflare's business.\\n4. **Competition**: It's important to assess how Cloudflare compares to its competitors in the market.\\n5. **Future Growth Potential**: Analyzing Cloudflare's growth strategies and potential for future expansion can provide insights into its investment potential.\\n\\nIt's recommended to conduct further research, consider your investment goals and risk tolerance before making any investment decisions.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11-5. Search Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    "    )  # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"The query you will search for\")\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"Use this tool to find the stock market symbol of a company.\"\n",
    "    args_schema: Type[StockMarketSymbolSearchToolArgsSchema] = (\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    )\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query=query)\n",
    "\n",
    "\n",
    "# 에이전트 초기화\n",
    "agent = initialize_agent(\n",
    "    llm=llm,  # LLM 모델\n",
    "    verbose=True,  # 상세 모드\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,  # 에이전트 타입\n",
    "    handle_parsing_errors=True,  # 파싱 에러 처리\n",
    "    tools=[StockMarketSymbolSearchTool()],\n",
    ")\n",
    "\n",
    "prompt = \"Give me information on Cloudflare stock and help me analyze if it's a potential good investment. Also tell me what symbol does the stock have.\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `StockMarketSymbolSearchTool` with `{'query': 'Plug Power'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mLATHAM, N.Y., July 18, 2024 (GLOBE NEWSWIRE) -- Plug Power Inc. (\"Plug Power\") (NASDAQ: PLUG), a global leader in comprehensive hydrogen solutions for the green hydrogen economy, today ... After Plug Power (PLUG-8.53%) stock plummeted 64% in 2023, investors rang in the new year with renewed hope that the fuel cell and hydrogen stock would be able to rebound from the previous year's ... Plug delivers its green hydrogen solutions directly to its customers and through joint venture partners into multiple environments, including material handling, e-mobility, power generation, and industrial applications. NASDAQ: PLUG. 3.21. +0.14 (4.56%) Volume: 60,403,759. 20 minutes minimum delay | July 15, 2024 | 4:00 PM. Plug Power Inc. (NASDAQ:PLUG) is active in a growth market, but the company's execution remains sub-par. The company burns through cash at a rapid pace, which increases the risk of ongoing ... Plug Power Secures Cryogenic Trailer and LNG Equipment Deals in North America. Read More ». Posted: May 2, 2024 From: Benzinga. Plug Power Eyes Korean Expansion? Secures First International Certification for Electrolyzer Manufacturing in the Country.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CompanyHistory` with `{'symbol': 'PLUG'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m                           Open  High   Low  ...     Volume  Dividends  Stock Splits\n",
      "Date                                         ...                                    \n",
      "2024-06-20 00:00:00-04:00  2.65  2.66  2.52  ...   24709400        0.0           0.0\n",
      "2024-06-21 00:00:00-04:00  2.55  2.56  2.40  ...   44582100        0.0           0.0\n",
      "2024-06-24 00:00:00-04:00  2.43  2.47  2.33  ...   25994900        0.0           0.0\n",
      "2024-06-25 00:00:00-04:00  2.50  2.53  2.35  ...   33964900        0.0           0.0\n",
      "2024-06-26 00:00:00-04:00  2.37  2.46  2.35  ...   21131900        0.0           0.0\n",
      "2024-06-27 00:00:00-04:00  2.48  2.51  2.41  ...   27291500        0.0           0.0\n",
      "2024-06-28 00:00:00-04:00  2.59  2.63  2.31  ...  121643400        0.0           0.0\n",
      "2024-07-01 00:00:00-04:00  2.35  2.40  2.26  ...   30377100        0.0           0.0\n",
      "2024-07-02 00:00:00-04:00  2.31  2.35  2.21  ...   29355800        0.0           0.0\n",
      "2024-07-03 00:00:00-04:00  2.30  2.53  2.29  ...   41364700        0.0           0.0\n",
      "2024-07-05 00:00:00-04:00  2.47  2.62  2.43  ...   29380900        0.0           0.0\n",
      "2024-07-08 00:00:00-04:00  2.63  2.71  2.55  ...   25754700        0.0           0.0\n",
      "2024-07-09 00:00:00-04:00  2.65  2.67  2.51  ...   24241700        0.0           0.0\n",
      "2024-07-10 00:00:00-04:00  2.60  2.72  2.55  ...   30404600        0.0           0.0\n",
      "2024-07-11 00:00:00-04:00  2.80  2.95  2.76  ...   43274000        0.0           0.0\n",
      "2024-07-12 00:00:00-04:00  3.05  3.14  2.97  ...   46198100        0.0           0.0\n",
      "2024-07-15 00:00:00-04:00  2.98  3.23  2.83  ...   60545200        0.0           0.0\n",
      "2024-07-16 00:00:00-04:00  3.21  3.34  3.08  ...   50762000        0.0           0.0\n",
      "2024-07-17 00:00:00-04:00  3.20  3.34  3.00  ...   42473500        0.0           0.0\n",
      "2024-07-18 00:00:00-04:00  3.10  3.23  2.91  ...   36325000        0.0           0.0\n",
      "\n",
      "[20 rows x 7 columns]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CompanyIncomeStatement` with `{'symbol': 'PLUG'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m                                                      2023-12-31  ...  2019-12-31\n",
      "Tax Effect Of Unusual Items                           -1504515.0  ...         NaN\n",
      "Tax Rate For Calcs                                         0.005  ...         NaN\n",
      "Normalized EBITDA                                   -959010000.0  ...         NaN\n",
      "Total Unusual Items                                 -300903000.0  ...         NaN\n",
      "Total Unusual Items Excluding Goodwill              -300903000.0  ...         NaN\n",
      "Net Income From Continuing Operation Net Minori... -1368833000.0  ...         NaN\n",
      "Reconciled Depreciation                               71083000.0  ...         NaN\n",
      "Reconciled Cost Of Revenue                          1399131000.0  ...         NaN\n",
      "EBITDA                                             -1259913000.0  ...         NaN\n",
      "EBIT                                               -1330996000.0  ...         NaN\n",
      "Net Interest Income                                   10628000.0  ...         NaN\n",
      "Interest Expense                                      45201000.0  ...         NaN\n",
      "Interest Income                                       55829000.0  ...         NaN\n",
      "Normalized Income                                  -1069434515.0  ...         NaN\n",
      "Net Income From Continuing And Discontinued Ope... -1368833000.0  ...         NaN\n",
      "Total Expenses                                      1935345000.0  ...         NaN\n",
      "Total Operating Income As Reported                 -1343523000.0  ...         NaN\n",
      "Diluted Average Shares                               595468419.0  ...         NaN\n",
      "Basic Average Shares                                 595468419.0  ...         NaN\n",
      "Diluted EPS                                                 -2.3  ...         NaN\n",
      "Basic EPS                                                   -2.3  ...         NaN\n",
      "Diluted NI Availto Com Stockholders                -1368833000.0  ...         NaN\n",
      "Net Income Common Stockholders                     -1368833000.0  ...         NaN\n",
      "Preferred Stock Dividends                                    NaN  ...   1812000.0\n",
      "Net Income                                         -1368833000.0  ...         NaN\n",
      "Net Income Including Noncontrolling Interests      -1368833000.0  ...         NaN\n",
      "Net Income Continuous Operations                   -1368833000.0  ...         NaN\n",
      "Tax Provision                                         -7364000.0  ...         NaN\n",
      "Pretax Income                                      -1376197000.0  ...         NaN\n",
      "Other Income Expense                                -342820000.0  ...         NaN\n",
      "Other Non Operating Income Expenses                    -131000.0  ...         NaN\n",
      "Special Income Charges                              -299518000.0  ...         NaN\n",
      "Other Special Charges                                        NaN  ...    518000.0\n",
      "Impairment Of Capital Assets                         269494000.0  ...         NaN\n",
      "Restructuring And Mergern Acquisition                 30024000.0  ...         NaN\n",
      "Earnings From Equity Interest                        -41786000.0  ...         NaN\n",
      "Gain On Sale Of Security                              -1385000.0  ...     79000.0\n",
      "Net Non Operating Interest Income Expense             10628000.0  ...         NaN\n",
      "Total Other Finance Cost                                     NaN  ...  35502000.0\n",
      "Interest Expense Non Operating                        45201000.0  ...         NaN\n",
      "Interest Income Non Operating                         55829000.0  ...         NaN\n",
      "Operating Income                                   -1044005000.0  ...         NaN\n",
      "Operating Expense                                    536214000.0  ...         NaN\n",
      "Research And Development                             113745000.0  ...         NaN\n",
      "Selling General And Administration                   422469000.0  ...         NaN\n",
      "Gross Profit                                        -507791000.0  ...         NaN\n",
      "Cost Of Revenue                                     1399131000.0  ...         NaN\n",
      "Total Revenue                                        891340000.0  ...         NaN\n",
      "Operating Revenue                                    880503000.0  ...         NaN\n",
      "\n",
      "[49 rows x 5 columns]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CompanyBalanceSheet` with `{'symbol': 'PLUG'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m                                                   2023-12-31  ...   2019-12-31\n",
      "Treasury Shares Number                             19169366.0  ...          NaN\n",
      "Ordinary Shares Number                            606135659.0  ...          NaN\n",
      "Share Issued                                      625305025.0  ...          NaN\n",
      "Net Debt                                           64156000.0  ...  143174000.0\n",
      "Total Debt                                        968850000.0  ...          NaN\n",
      "...                                                       ...  ...          ...\n",
      "Allowance For Doubtful Accounts Receivable         -8798000.0  ...          NaN\n",
      "Gross Accounts Receivable                         252609000.0  ...          NaN\n",
      "Cash Cash Equivalents And Short Term Investments  135033000.0  ...          NaN\n",
      "Other Short Term Investments                              0.0  ...          NaN\n",
      "Cash And Cash Equivalents                         135033000.0  ...          NaN\n",
      "\n",
      "[83 rows x 5 columns]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CompanyCashflow` with `{'symbol': 'PLUG'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m                                           2023-12-31  ... 2019-12-31\n",
      "Free Cash Flow                          -1802696000.0  ...        NaN\n",
      "Repurchase Of Capital Stock                       NaN  ... -4040000.0\n",
      "Repayment Of Debt                         -79635000.0  ...        NaN\n",
      "Issuance Of Debt                          104251000.0  ...        NaN\n",
      "Issuance Of Capital Stock                         0.0  ...        NaN\n",
      "...                                               ...  ...        ...\n",
      "Operating Gains Losses                     54592000.0  ...        NaN\n",
      "Earnings Losses From Equity Investments    41786000.0  ...        NaN\n",
      "Gain Loss On Investment Securities         12806000.0  ...        NaN\n",
      "Gain Loss On Sale Of PPE                          NaN  ...   212000.0\n",
      "Net Income From Continuing Operations   -1368833000.0  ...        NaN\n",
      "\n",
      "[68 rows x 5 columns]\u001b[0m\u001b[32;1m\u001b[1;3m### Financial Analysis of Plug Power Inc. (NASDAQ: PLUG)\n",
      "\n",
      "#### Stock History\n",
      "Plug Power's stock has shown significant volatility in recent times. The stock price has fluctuated between $2.21 and $3.34 over the last month, indicating a potentially high-risk investment.\n",
      "\n",
      "#### Income Statement Highlights (2023)\n",
      "- **Net Income**: The company reported a substantial net loss of $1.37 billion.\n",
      "- **Revenue**: Total revenue was $891.34 million.\n",
      "- **Operating Income**: Operating loss was $1.04 billion.\n",
      "- **EBITDA**: Negative $1.26 billion, indicating operational challenges.\n",
      "- **Cost of Revenue**: $1.39 billion, which is higher than the gross profit, suggesting cost management issues.\n",
      "\n",
      "#### Balance Sheet Highlights (2023)\n",
      "- **Total Debt**: $968.85 million, with a net debt position of $64.16 million.\n",
      "- **Cash and Equivalents**: $135.03 million, which provides some liquidity but may be insufficient to cover the debt fully.\n",
      "- **Assets vs. Liabilities**: The company has a significant amount of total debt compared to its cash reserves, which could pose financial stability risks.\n",
      "\n",
      "#### Cash Flow Highlights (2023)\n",
      "- **Free Cash Flow**: Negative $1.80 billion, indicating that the company is burning through cash rather than generating it.\n",
      "- **Cash Flow from Operations**: Negative, which is concerning as it suggests that the core operations are not generating sufficient cash.\n",
      "\n",
      "### Investment Considerations\n",
      "- **High Risk of Volatility**: The stock's recent price history shows high volatility, which might appeal to speculative traders but could be risky for long-term investors.\n",
      "- **Financial Health Concerns**: The negative free cash flow and substantial net loss are red flags. The company's ability to sustain operations without additional financing could be in question.\n",
      "- **Growth vs. Profitability**: While Plug Power is active in the growth market of green hydrogen solutions, its current financials reflect poor execution and profitability challenges.\n",
      "\n",
      "### Conclusion\n",
      "Investing in Plug Power involves significant risk. The company's financial health is currently weak, with high losses and negative cash flows. Potential investors should consider this a speculative investment and weigh the possibility of future growth against the evident financial challenges. It might be suitable for those with a high risk tolerance and a long-term investment horizon who believe in the future of hydrogen energy. However, conservative investors might find the current financial instability a deterrent.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### Financial Analysis of Plug Power Inc. (NASDAQ: PLUG)\\n\\n#### Stock History\\nPlug Power's stock has shown significant volatility in recent times. The stock price has fluctuated between $2.21 and $3.34 over the last month, indicating a potentially high-risk investment.\\n\\n#### Income Statement Highlights (2023)\\n- **Net Income**: The company reported a substantial net loss of $1.37 billion.\\n- **Revenue**: Total revenue was $891.34 million.\\n- **Operating Income**: Operating loss was $1.04 billion.\\n- **EBITDA**: Negative $1.26 billion, indicating operational challenges.\\n- **Cost of Revenue**: $1.39 billion, which is higher than the gross profit, suggesting cost management issues.\\n\\n#### Balance Sheet Highlights (2023)\\n- **Total Debt**: $968.85 million, with a net debt position of $64.16 million.\\n- **Cash and Equivalents**: $135.03 million, which provides some liquidity but may be insufficient to cover the debt fully.\\n- **Assets vs. Liabilities**: The company has a significant amount of total debt compared to its cash reserves, which could pose financial stability risks.\\n\\n#### Cash Flow Highlights (2023)\\n- **Free Cash Flow**: Negative $1.80 billion, indicating that the company is burning through cash rather than generating it.\\n- **Cash Flow from Operations**: Negative, which is concerning as it suggests that the core operations are not generating sufficient cash.\\n\\n### Investment Considerations\\n- **High Risk of Volatility**: The stock's recent price history shows high volatility, which might appeal to speculative traders but could be risky for long-term investors.\\n- **Financial Health Concerns**: The negative free cash flow and substantial net loss are red flags. The company's ability to sustain operations without additional financing could be in question.\\n- **Growth vs. Profitability**: While Plug Power is active in the growth market of green hydrogen solutions, its current financials reflect poor execution and profitability challenges.\\n\\n### Conclusion\\nInvesting in Plug Power involves significant risk. The company's financial health is currently weak, with high losses and negative cash flows. Potential investors should consider this a speculative investment and weigh the possibility of future growth against the evident financial challenges. It might be suitable for those with a high risk tolerance and a long-term investment horizon who believe in the future of hydrogen energy. However, conservative investors might find the current financial instability a deterrent.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11-6. Stock Information Tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import initialize_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type\n",
    "import os\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "alpha_ventage_api_key = os.environ.get(\"ALPHA_VENTAGE_API_KEY\")\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    temperature=0.1\n",
    ")  # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"The query you will search for\")\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"Use this tool to find the stock market symbol of a company.\"\n",
    "    args_schema: Type[StockMarketSymbolSearchToolArgsSchema] = (\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    )\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query=query)\n",
    "\n",
    "class CampayHistoryToolArgsSchema(BaseModel):\n",
    "    symbol: str = Field(description=\"Stock symbol of the company.\\nExample:APPL,TSLA\")\n",
    "\n",
    "class CompanyHistoryTool(BaseTool):\n",
    "    name = \"CompanyHistory\"\n",
    "    description = \"\"\"\n",
    "    Use this to get an history of the financials of the company.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CampayHistoryToolArgsSchema] = CampayHistoryToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        c = yf.Ticker(symbol)\n",
    "        return c.history(period=\"1mo\")\n",
    "\n",
    "\n",
    "class CompanyIncomeStatementTool(BaseTool):\n",
    "    name = \"CompanyIncomeStatement\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the income statement of a company.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CampayHistoryToolArgsSchema] = CampayHistoryToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        c = yf.Ticker(symbol)\n",
    "        return c.income_stmt\n",
    "\n",
    "\n",
    "class CompanyBalanceSheetTool(BaseTool):\n",
    "    name = \"CompanyBalanceSheet\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the balance sheet of a company stock.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CampayHistoryToolArgsSchema] = CampayHistoryToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        c = yf.Ticker(symbol)\n",
    "        return c.balance_sheet\n",
    "\n",
    "class CompanyCashflowTool(BaseTool):\n",
    "    name = \"CompanyCashflow\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the cashflow of a company stock.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CampayHistoryToolArgsSchema] = CampayHistoryToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        c = yf.Ticker(symbol)\n",
    "        return c.cashflow\n",
    "\n",
    "\n",
    "# 에이전트 초기화\n",
    "agent = initialize_agent(\n",
    "    llm=llm,  # LLM 모델\n",
    "    verbose=True,  # 상세 모드\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,  # 에이전트 타입\n",
    "    handle_parsing_errors=True,  # 파싱 에러 처리\n",
    "    tools=[\n",
    "        StockMarketSymbolSearchTool(),\n",
    "        CompanyHistoryTool(),\n",
    "        CompanyIncomeStatementTool(),\n",
    "        CompanyBalanceSheetTool(),\n",
    "        CompanyCashflowTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Give me financial information on Cloudflare stock,\n",
    "considering it's history, financials, income statements, balance sheet and cashflow help me analyze if it's a potential good investment.\n",
    "\"\"\"\n",
    "\n",
    "result = agent.invoke(prompt)\n",
    "\n",
    "result[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
