{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nThere are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.',\n",
       " AIMessage(content='There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 16, 'total_tokens': 43}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2e9ccd02-200b-49b5-9220-6469d39b5e18-0', usage_metadata={'input_tokens': 16, 'output_tokens': 27, 'total_tokens': 43}))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-1 LLM and Chat Models\n",
    "from langchain_openai import OpenAI, ChatOpenAI # LLM, Chat model\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "a = llm.invoke(\"How many planets are in the solar system?\")\n",
    "b = chat.invoke(\"How many planets are in the solar system?\")\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri. Come ti chiami?', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 53, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d65532a0-253a-4c36-8e87-308c9259c095-0', usage_metadata={'input_tokens': 53, 'output_tokens': 31, 'total_tokens': 84})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2-2. Predict Messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "# HumanMessage - 인간이 작성하는 Message\n",
    "# AIMessage - AI에 의해서 보내지는 Message\n",
    "# SystemMessage - LLM에 설정들을 제공하기 위한 Message\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italian.\"),\n",
    "    AIMessage(content=\"Ciao, mi chiamo Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between the Mexico and Thailand. Also, what is your name?\"),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri. Come ti chiami?', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 53, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d65532a0-253a-4c36-8e87-308c9259c095-0', usage_metadata={'input_tokens': 53, 'output_tokens': 31, 'total_tokens': 84})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2-3. Prompt Templates\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# 일반 문자열 출력 예제\n",
    "# template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}\")\n",
    "# prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "# chat.predict(prompt)\n",
    "\n",
    "# 메시지 출력 예제\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a geography expert. And you only reply in {language}.\"),\n",
    "    AIMessagePromptTemplate.from_template(\"Ciao, mi chiamo {name}!\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"What is the distance between the {country_a} and {country_b}. Also, what is your name?\")\n",
    "]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "prompt = template.format_messages(language=\"Italian\", name=\"Paolo\", country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-4. OutputParser and LCEL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# 문자열 출력을 파싱하는 BaseOutputParser 확장하는 커스텀 OutputParser\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "\n",
    "# OutputParser 예제 (LCEL 적용 전)\n",
    "# p = CommaOutputParser()\n",
    "# messages = [\n",
    "#     SystemMessagePromptTemplate.from_template(\"You are a list gernerating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do Not reply with else.\"),\n",
    "#     HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "# ]\n",
    "# template = ChatPromptTemplate.from_messages(messages)\n",
    "# prompt = template.format_messages(max_items=10, question=\"What are the colors?\")\n",
    "# res = chat.invoke(prompt)\n",
    "# p.parse(res.content)\n",
    "\n",
    "# OutputParser 예제 (LCEL 적용 후)\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a list gernerating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do Not reply with else.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "template = ChatPromptTemplate.from_messages(messages)\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\":10, \n",
    "    \"question\":\"What are the colors?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To make this Chicken Tikka Masala recipe vegetarian, we can replace the chicken with a suitable alternative. Here's how you can modify the recipe:\\n\\n**Vegetarian Tikka Masala**\\n\\n**Ingredients:**\\n- 1 lb firm tofu, pressed and cut into bite-sized cubes\\n- 1 cup plain yogurt (you can use plant-based yogurt for a vegan version)\\n- 2 tablespoons lemon juice\\n- 2 teaspoons ground cumin\\n- 2 teaspoons paprika\\n- 1 teaspoon ground turmeric\\n- 1 teaspoon garam masala\\n- 1 teaspoon ground coriander\\n- 1 teaspoon chili powder (adjust to taste)\\n- Salt and pepper to taste\\n- 2 tablespoons vegetable oil\\n- 1 onion, finely chopped\\n- 3 cloves garlic, minced\\n- 1-inch piece of ginger, grated\\n- 1 can (14 oz) crushed tomatoes\\n- 1 cup coconut cream (or heavy cream alternative for a vegan version)\\n- Fresh cilantro, chopped (for garnish)\\n\\n**Instructions:**\\n1. In a bowl, combine yogurt, lemon juice, cumin, paprika, turmeric, garam masala, coriander, chili powder, salt, and pepper. Add the tofu cubes and coat them well with the marinade. Cover and refrigerate for at least 1 hour, or overnight for best results.\\n\\n2. Preheat the oven to 400°F (200°C). Thread the marinated tofu onto skewers and place them on a baking sheet lined with foil. Bake for 20-25 minutes or until the tofu is lightly browned.\\n\\n3. In a large skillet, heat vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes. Add the garlic and ginger, and cook for another minute until fragrant.\\n\\n4. Stir in the crushed tomatoes and simmer for 10 minutes. Add the coconut cream and cooked tofu tikka pieces. Simmer for an additional 10 minutes, stirring occasionally.\\n\\n5. Taste and adjust seasoning if needed. Garnish with chopped cilantro before serving.\\n\\n6. Serve the Vegetarian Tikka Masala hot with steamed rice or naan bread.\\n\\nEnjoy your flavorful and aromatic Vegetarian Tikka Masala!\", response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-93f9ef73-d490-4866-878e-4be3b72f0756-0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-5. Chaining Chains\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "chef_message =  [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a world-class international chef. You create easy to fllow recipies for any type of cuisine with easy to find ingredients.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"I want to cook {cuisine} food.\")\n",
    "]\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages(chef_message)\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "veg_chef_message =  [\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a vegetarian chef specialized on marking tranditional recipies vegetarian. You find alternatibe ingredients and explain their preparation. You don't redically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{recipe}\")\n",
    "]\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages(veg_chef_message)\n",
    "\n",
    "veg_chef_chain = veg_chef_prompt | chat\n",
    "\n",
    "# RunnableMap 사용\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chef_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\":\"indian\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: \\n        I\\'m sorry, I\\'m not sure what you mean by \"Germ.\" Did you mean Germany?', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-a639704b-2eb3-4730-8a41-3c3a437becde-0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1. FewShotPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# 모델에게 전달하는 답변 예제\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    }]\n",
    "\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt, # Prompt 방식\n",
    "    examples=examples, # 답변 예제\n",
    "    suffix=\"Human: Wat do you know about {country}?\", # 모든 형식화된 예제 마지막 내용\n",
    "    input_variables=[\"country\"] # suffix 입력 변수 (유효성 검사)\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Germ\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n        I know this:\\n        Capital: Berlin\\n        Language: German\\n        Food: Bratwurst and Sauerkraut\\n        Currency: Euro\\n        ', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-213f3cfb-0c41-4b51-aab7-5925964cf6d6-0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-2. FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# 모델에게 전달하는 답변 예제\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    }]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"What do you know about {country}?\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{answer}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt, # Prompt 방식\n",
    "    examples=examples, # 답변 예제\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a geography expert, you give short answers\"),\n",
    "    prompt,\n",
    "    HumanMessagePromptTemplate.from_template(\"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Germany\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuman: Wat do you know about Brazil?'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-3. LengthBasedExampleSelector\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# 모델에게 전달하는 답변 예제\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    }]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# 예제 선택\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples, # 답변 예제\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt, # Prompt 방식\n",
    "    example_selector=example_selector, # 답변 선택\n",
    "    suffix=\"Human: Wat do you know about {country}?\", # 모든 형식화된 예제 마지막 내용\n",
    "    input_variables=[\"country\"] # suffix 입력 변수 (유효성 검사)\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrrrg! Me favorite food be a hearty stew made with fresh seafood and plenty of spices! Aye, it be a meal fit for a pirate like meself! Arrrrg!', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-24515c0d-3d20-4b5c-8ffb-4b6a73447353-0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-4. Serialization and Composition\n",
    "from langchain_openai import ChatOpenAI\n",
    "# PromptTemplate - 문자열을 이용한 template 생성\n",
    "# ChatPromptTemplate - message를 이용하여 template 생성\n",
    "from langchain.prompts import PromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# Prompt 파일 불러오기\n",
    "# from langchain.prompts import load_prompt\n",
    "# Prompt Pipeline 불러오기\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "# Prompt 파일 불러오기\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "# prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "# Prompt 파일 불러오기\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "# full_prompt.format(character=\"Pirate\", example_question=\"What is your location?\", example_answer=\"Arrrrg! That is a secret!! Arg arg!!\", question=\"What is your fav food?\")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\":\"Pirate\",\n",
    "    \"example_question\":\"What is your location?\",\n",
    "    \"example_answer\":\"Arrrrg! That is a secret!! Arg arg\",\n",
    "    \"question\": \"What is your fav food?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss with your favorite sauce or toppings.\\n10. Serve hot and enjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-5. Caching\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# 메모리 캐시\n",
    "# set_llm_cache(InMemoryCache())\n",
    "# set_debug(False)\n",
    "\n",
    "# 데이터베이스 캐시\n",
    "set_llm_cache(SQLiteCache('cache.db'))\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "    # streaming=True, # streaming 옵션을 활성화하여 대화형 모드로 설정\n",
    "    # callbacks=[StreamingStdOutCallbackHandler()], # 콜백 함수를 설정\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAIChat\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 450}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seo/project/gpt/env/lib/python3.12/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/seo/project/gpt/env/lib/python3.12/site-packages/langchain_community/llms/openai.py:1072: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3-6. Serialization\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "# )\n",
    "\n",
    "# 지출 비용 확인\n",
    "# with get_openai_callback() as usage:\n",
    "#     a = chat.predict(\"What is the recipe for soju?\")\n",
    "#     b = chat.predict(\"What is the recipe for bread?\")\n",
    "#     print(a, b, \"\\n\")\n",
    "#     print(usage)\n",
    "\n",
    "# Serialization\n",
    "# 모델 저장\n",
    "# chat = OpenAI(\n",
    "#     temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    "#     max_tokens=450,\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "# )\n",
    "\n",
    "# chat.save('model.json')\n",
    "\n",
    "# 모델 불러오기\n",
    "chat = load_llm('model.json')\n",
    "\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-1. ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 쳇 모델 용으로 사용할 경우 return_messages=True로 설정\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-2. ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 쳇 모델 용으로 사용할 경우 return_messages=True로 설정, K는 메모리의 크기\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "add_message(5, 5)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as living in South Korea, and the AI responds by mentioning it is a robot living in the cloud. The human comments on the beauty of South Korea, and the AI expresses a wish to visit there.'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-3. ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=chat)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"HI I am a human, I live in South Korea\", \"Wow that is cool! I am a robot living in the cloud.\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could visit there.\")\n",
    "\n",
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as living in South Korea. The AI responds by mentioning it is a robot living in the cloud and expresses a desire to visit South Korea because it is so pretty.')]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-4. ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"HI I am a human, I live in South Korea\", \"Wow that is cool! I am a robot living in the cloud.\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could visit there.\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Lee: Lee lives in South Korea. Lee likes kimchi.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-5. ConversationKGMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationKGMemory(\n",
    "    llm=chat,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"HI I am a Lee, I live in South Korea\", \"Wow that is cool! I am a robot living in the cloud.\")\n",
    "add_message(\"Lee likes kimchi\", \"wow that is cool!\")\n",
    "\n",
    "memory.load_memory_variables({\"input\": \"who is Lee\"})\n",
    "memory.load_memory_variables({\"input\": \"what does Lee like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Seo\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Seo\n",
      "AI: Hello Seo! How can I assist you today?\n",
      "    Human:I live Seoul in South Korea\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Seo\n",
      "AI: Hello Seo! How can I assist you today?\n",
      "Human: I live Seoul in South Korea\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul or South Korea?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: My name is Seo\\nAI: Hello Seo! How can I assist you today?\\nHuman: I live Seoul in South Korea\\nAI: That's great to know! How can I assist you with information or tasks related to Seoul or South Korea?\\nHuman: What is my name?\\nAI: Your name is Seo.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-6. Memory on LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Seo\")\n",
    "chain.predict(question=\"I live Seoul in South Korea\") \n",
    "chain.predict(question=\"What is my name?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Seo\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Seo\n",
      "AI: Nice to meet you, Seo! How can I assist you today?\n",
      "Human: I live Seoul in South Korea\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Seo\n",
      "AI: Nice to meet you, Seo! How can I assist you today?\n",
      "Human: I live Seoul in South Korea\n",
      "AI: That's great to know, Seo! Is there anything specific you would like to know or talk about regarding Seoul or South Korea?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Seo.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-7. Chat Based Memory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 모델의 창의성을 조절하는 옵션 (높을 수록 창의적임)\n",
    ")\n",
    "\n",
    "# max_token_limit은 메모리에 저장할 최대 토큰 수\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # 많은 메시지들 사이를 구분하기 위한 placeholder\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True, \n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Seo\")\n",
    "chain.predict(question=\"I live Seoul in South Korea\") \n",
    "chain.predict(question=\"What is my name?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-8. LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-1. Data Loaders and Splitters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
